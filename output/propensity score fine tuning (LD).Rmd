---
title: "propensity score tuning (LD)"
author: "Olha Maslova"
date: "3/28/2021"
output: html_document
Source: https://uc-r.github.io/regression_trees
---

## R Markdown
```{r}
setwd("/Users/my_love/Desktop/Applied Data Science/Project 4/Spring2021-Project4-project4group1/")
data <- read.csv(file = "data/lowDim_dataset.csv")
data <- subset(data, select = -Y)
```
# Data Import
```{r}
library(rsample)     # data splitting 
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(caret)
```

### Train test split
```{r}
set.seed(123)
data_split <- initial_split(data, prop = .7)
train <- training(data_split)
test  <- testing(data_split)
```

### Build first model
```{r}
m1 <- rpart(
  formula = A ~ .,
  data    = train,
  method  = "anova"
)
```

### Plot the tree
```{r}
rpart.plot(m1)
```

### Plot cross validated error summary
```{r}
plotcp(m1)
```

### Force rpart to generate a full tree by using
```{r}
m2 <- rpart(
  formula = A ~ .,
  data    = train,
  method  = "anova",
  control = list(cp = 0, xval = 10)
)
```

### Plot the tree
```{r}
rpart.plot(m2)
```

### Plot cross validated error summary
```{r}
plotcp(m2)
abline(v = 16, lty = "dashed")
```

We can see that after 13 terminal nodes, we see diminishing returns in error reduction as the tree grows deeper. Thus, we can significantly prune our tree and still achieve minimal expected error.

## Tuning
### Create a grid search
```{r}
hyper_grid <- expand.grid(
  minsplit = seq(5, 20, 1),
  maxdepth = seq(8, 20, 1)
)
```

### Search the created space for the best params
```{r}
models <- list()

for (i in 1:nrow(hyper_grid)) {
  
  # get minsplit, maxdepth values at row i
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]
  
  # train a model and store in the list
  models[[i]] <- rpart(
    formula = A ~ .,
    data    = train,
    method  = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
  )
}
```

### Function to get optimal cp
```{r}
get_cp <- function(x) {
  min <- which.min(x$cptable[, "xerror"])
  cp <- x$cptable[min, "CP"] 
}
```

### function to get minimum error
```{r}
get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"] 
}
```

### Find best params
```{r}
hyper_grid %>%
  mutate(
    cp    = purrr::map_dbl(models, get_cp),
    error = purrr::map_dbl(models, get_min_error)
  ) %>%
  arrange(error) %>%
  top_n(-5, wt = error)
```

### Optimal Tree and its RMSE
```{r}
optimal_tree <- rpart(
  formula = A ~ .,
  data    = train,
  method  = "anova",
  control = list(minsplit = 5, maxdepth = 16, cp = 0.0258)
)

pred <- predict(optimal_tree, newdata = test)
RMSE(pred = pred, obs = test$A)
```

