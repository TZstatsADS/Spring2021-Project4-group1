---
title: "propensity score tuning (LD)"
author: "Olha Maslova"
date: "3/28/2021"
output: html_document
Source: https://uc-r.github.io/regression_trees
---

## R Markdown
```{r}
setwd("/Users/my_love/Desktop/Applied Data Science/Project 4/Spring2021-Project4-project4group1/")
data <- read.csv(file = "data/lowDim_dataset.csv")
data <- subset(data, select = -Y)
```
# Data Import
```{r}
library(rsample)     # data splitting 
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(caret)
```

### Train test split
```{r}
set.seed(123)
data_split <- initial_split(data, prop = .7)
train <- training(data_split)
test  <- testing(data_split)
```

### Build first model
```{r}
m1 <- rpart(
  formula = A ~ .,
  data    = train,
)
```

### Plot the tree
```{r}
rpart.plot(m1)
```

### Force rpart to generate a full tree by using cp=0
```{r}
m2 <- rpart(
  formula = A ~ .,
  data    = train,
  method  = "class",
  control = list(cp = 0, xval = 10)
)
```

### Plot the tree
```{r}
rpart.plot(m2)
```

## Tuning
### Create a grid search
```{r}
hyper_grid <- expand.grid(
  minsplit = seq(5, 20, 1),
  maxdepth = seq(8, 20, 1)
)
```

### Search the created space for the best params
```{r}
models <- list()

for (i in 1:nrow(hyper_grid)) {
  
  # get minsplit, maxdepth values at row i
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]
  
  # train a model and store in the list
  models[[i]] <- rpart(
    formula = A ~ .,
    data    = train,
    method  = "class",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
  )
}
```

### Function to get optimal cp
```{r}
get_cp <- function(x) {
  min <- which.min(x$cptable[, "xerror"])
  cp <- x$cptable[min, "CP"] 
}
```

### function to get minimum error
```{r}
get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"] 
}
```

### Find best params
```{r}
hyper_grid %>%
  mutate(
    cp    = purrr::map_dbl(models, get_cp),
    error = purrr::map_dbl(models, get_min_error)
  ) %>%
  arrange(error) %>%
  top_n(-5, wt = error)
```

### Optimal Tree and its RMSE
```{r}
optimal_tree_ld <- rpart(
  formula = A ~ .,
  data    = train,
  method  = "class",
  control = list(minsplit = 11, maxdepth = 14)
)

pred <- predict(optimal_tree_ld, newdata = data)
```

```{r}
hist(pred[, 2], main='Histogram of probabilities that individual belongs to class 1', xlab = 'Probability')
```

```{r}
rpart.plot(optimal_tree_ld)
```
