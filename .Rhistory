plotcp(m1)
m2 <- rpart(
formula = Y ~ .,
data    = low_dim_train,
method  = "anova",
control = list(cp = 0, xval = 10)
)
rpart.plot(m2)
plotcp(m2)
abline(v = 13, lty = "dashed")
hyper_grid <- expand.grid(
minsplit = seq(5, 20, 1),
maxdepth = seq(8, 15, 1)
)
models <- list()
for (i in 1:nrow(hyper_grid)) {
# get minsplit, maxdepth values at row i
minsplit <- hyper_grid$minsplit[i]
maxdepth <- hyper_grid$maxdepth[i]
# train a model and store in the list
models[[i]] <- rpart(
formula = Y ~ .,
data    = low_dim_train,
method  = "anova",
control = list(minsplit = minsplit, maxdepth = maxdepth)
)
}
get_cp <- function(x) {
min <- which.min(x$cptable[, "xerror"])
cp <- x$cptable[min, "CP"]
}
get_min_error <- function(x) {
min    <- which.min(x$cptable[, "xerror"])
xerror <- x$cptable[min, "xerror"]
}
hyper_grid %>%
mutate(
cp    = purrr::map_dbl(models, get_cp),
error = purrr::map_dbl(models, get_min_error)
) %>%
arrange(error) %>%
top_n(-5, wt = error)
optimal_tree <- rpart(
formula = Y ~ .,
data    = low_dim_train,
method  = "anova",
control = list(minsplit = 10, maxdepth = 10, cp = 0.01)
)
pred <- predict(optimal_tree, newdata = low_dim_test)
RMSE(pred = pred, obs = low_dim_test$Y)
setwd("/Users/my_love/Desktop/Applied Data Science/Project 4/Spring2021-Project4-project4group1/")
data <- read.csv(file = "data/highDim_dataset.csv")
library(rsample)     # data splitting
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
set.seed(123)
data_split <- initial_split(data, prop = .7)
train <- training(data_split)
test  <- testing(data_split)
train
m1 <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova"
)
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/my_love/Desktop/Applied Data Science/Project 4/Spring2021-Project4-project4group1/")
data <- read.csv(file = "data/lowDim_dataset.csv")
library(rsample)     # data splitting
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
set.seed(123)
data_split <- initial_split(data, prop = .7)
train <- training(data_split)
test  <- testing(data_split)
m1 <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova"
)
rpart.plot(m1)
plotcp(m1)
m2 <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(cp = 0, xval = 10)
)
rpart.plot(m2)
plotcp(m2)
abline(v = 13, lty = "dashed")
hyper_grid <- expand.grid(
minsplit = seq(5, 20, 1),
maxdepth = seq(8, 15, 1)
)
models <- list()
for (i in 1:nrow(hyper_grid)) {
# get minsplit, maxdepth values at row i
minsplit <- hyper_grid$minsplit[i]
maxdepth <- hyper_grid$maxdepth[i]
# train a model and store in the list
models[[i]] <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = minsplit, maxdepth = maxdepth)
)
}
get_cp <- function(x) {
min <- which.min(x$cptable[, "xerror"])
cp <- x$cptable[min, "CP"]
}
get_min_error <- function(x) {
min    <- which.min(x$cptable[, "xerror"])
xerror <- x$cptable[min, "xerror"]
}
hyper_grid %>%
mutate(
cp    = purrr::map_dbl(models, get_cp),
error = purrr::map_dbl(models, get_min_error)
) %>%
arrange(error) %>%
top_n(-5, wt = error)
optimal_tree <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = 10, maxdepth = 10, cp = 0.01)
)
pred <- predict(optimal_tree, newdata = test)
RMSE(pred = pred, obs = test$Y)
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/my_love/Desktop/Applied Data Science/Project 4/Spring2021-Project4-project4group1/")
data <- read.csv(file = "data/lowDim_dataset.csv")
library(rsample)     # data splitting
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
set.seed(123)
data_split <- initial_split(data, prop = .7)
train <- training(data_split)
test  <- testing(data_split)
m1 <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova"
)
rpart.plot(m1)
plotcp(m1)
m2 <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(cp = 0, xval = 10)
)
rpart.plot(m2)
plotcp(m2)
abline(v = 13, lty = "dashed")
hyper_grid <- expand.grid(
minsplit = seq(5, 20, 1),
maxdepth = seq(8, 15, 1)
)
models <- list()
for (i in 1:nrow(hyper_grid)) {
# get minsplit, maxdepth values at row i
minsplit <- hyper_grid$minsplit[i]
maxdepth <- hyper_grid$maxdepth[i]
# train a model and store in the list
models[[i]] <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = minsplit, maxdepth = maxdepth)
)
}
get_cp <- function(x) {
min <- which.min(x$cptable[, "xerror"])
cp <- x$cptable[min, "CP"]
}
get_min_error <- function(x) {
min    <- which.min(x$cptable[, "xerror"])
xerror <- x$cptable[min, "xerror"]
}
hyper_grid %>%
mutate(
cp    = purrr::map_dbl(models, get_cp),
error = purrr::map_dbl(models, get_min_error)
) %>%
arrange(error) %>%
top_n(-5, wt = error)
optimal_tree <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = 10, maxdepth = 10, cp = 0.01)
)
pred <- predict(optimal_tree, newdata = test)
RMSE(pred = pred, obs = test$Y)
m1 <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova"
)
rpart.plot(m1)
plotcp(m1)
m2 <- rpart(
formula = Y ~ .,
data    = low_dim_train,
method  = "anova",
control = list(cp = 0, xval = 10)
)
m2 <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(cp = 0, xval = 10)
)
rpart.plot(m2)
m2 <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(cp = 0, xval = 10)
)
rpart.plot(m2)
m2
train
setwd("/Users/my_love/Desktop/Applied Data Science/Project 4/Spring2021-Project4-project4group1/")
data <- read.csv(file = "data/highDim_dataset.csv")
set.seed(123)
data_split <- initial_split(data, prop = .7)
train <- training(data_split)
test  <- testing(data_split)
setwd("/Users/my_love/Desktop/Applied Data Science/Project 4/Spring2021-Project4-project4group1/")
data <- read.csv(file = "data/highDim_dataset.csv")
library(rsample)     # data splitting
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
library(rsample)     # data splitting
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
set.seed(123)
data_split <- initial_split(data, prop = .7)
train <- training(data_split)
test  <- testing(data_split)
m1 <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova"
)
rpart.plot(m1)
plotcp(m1)
m2 <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(cp = 0, xval = 10)
)
train
m1
plotcp(m1)
m2 <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(cp = 0, xval = 10)
)
rpart.plot(m2)
plotcp(m2)
abline(v = 13, lty = "dashed")
plotcp(m2)
abline(v = 7, lty = "dashed")
hyper_grid <- expand.grid(
minsplit = seq(5, 20, 1),
maxdepth = seq(8, 15, 1)
)
models <- list()
for (i in 1:nrow(hyper_grid)) {
# get minsplit, maxdepth values at row i
minsplit <- hyper_grid$minsplit[i]
maxdepth <- hyper_grid$maxdepth[i]
# train a model and store in the list
models[[i]] <- rpart(
formula = Y ~ .,
data    = low_dim_train,
method  = "anova",
control = list(minsplit = minsplit, maxdepth = maxdepth)
)
}
models <- list()
for (i in 1:nrow(hyper_grid)) {
# get minsplit, maxdepth values at row i
minsplit <- hyper_grid$minsplit[i]
maxdepth <- hyper_grid$maxdepth[i]
# train a model and store in the list
models[[i]] <- rpart(
formula = Y ~ .,
data    = ltrain,
method  = "anova",
control = list(minsplit = minsplit, maxdepth = maxdepth)
)
}
models <- list()
for (i in 1:nrow(hyper_grid)) {
# get minsplit, maxdepth values at row i
minsplit <- hyper_grid$minsplit[i]
maxdepth <- hyper_grid$maxdepth[i]
# train a model and store in the list
models[[i]] <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = minsplit, maxdepth = maxdepth)
)
}
get_cp <- function(x) {
min <- which.min(x$cptable[, "xerror"])
cp <- x$cptable[min, "CP"]
}
get_min_error <- function(x) {
min    <- which.min(x$cptable[, "xerror"])
xerror <- x$cptable[min, "xerror"]
}
hyper_grid %>%
mutate(
cp    = purrr::map_dbl(models, get_cp),
error = purrr::map_dbl(models, get_min_error)
) %>%
arrange(error) %>%
top_n(-5, wt = error)
optimal_tree <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = 15, maxdepth = 10, cp = 0.01)
)
pred <- predict(optimal_tree, newdata = test)
RMSE(pred = pred, obs = test$Y)
### Code lib Folder
The lib directory contains propensity_score.R file which has the definition for two fucntions:
1.
setwd("/Users/my_love/Desktop/Applied Data Science/Project 4/Spring2021-Project4-project4group1/")
data <- read.csv(file = "data/highDim_dataset.csv")
library(rsample)     # data splitting
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
set.seed(123)
data_split <- initial_split(data, prop = .7)
train <- training(data_split)
test  <- testing(data_split)
hyper_grid <- expand.grid(
minsplit = seq(5, 20, 1),
maxdepth = seq(8, 180, 1)
)
models <- list()
for (i in 1:nrow(hyper_grid)) {
# get minsplit, maxdepth values at row i
minsplit <- hyper_grid$minsplit[i]
maxdepth <- hyper_grid$maxdepth[i]
# train a model and store in the list
models[[i]] <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = minsplit, maxdepth = maxdepth)
)
}
get_cp <- function(x) {
min <- which.min(x$cptable[, "xerror"])
cp <- x$cptable[min, "CP"]
}
get_min_error <- function(x) {
min    <- which.min(x$cptable[, "xerror"])
xerror <- x$cptable[min, "xerror"]
}
hyper_grid %>%
mutate(
cp    = purrr::map_dbl(models, get_cp),
error = purrr::map_dbl(models, get_min_error)
) %>%
arrange(error) %>%
top_n(-5, wt = error)
optimal_tree <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = 14, maxdepth = 100, cp = 0.01)
)
pred <- predict(optimal_tree, newdata = test)
RMSE(pred = pred, obs = test$Y)
optimal_tree <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = 14, maxdepth = 100, cp = 0.01)
)
setwd("/Users/my_love/Desktop/Applied Data Science/Project 4/Spring2021-Project4-project4group1/")
data <- read.csv(file = "data/highDim_dataset.csv")
library(rsample)     # data splitting
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
set.seed(123)
data_split <- initial_split(data, prop = .7)
train <- training(data_split)
test  <- testing(data_split)
optimal_tree <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = 14, maxdepth = 100, cp = 0.01)
)
pred <- predict(optimal_tree, newdata = test)
RMSE(pred = pred, obs = test$Y)
optimal_tree <- rpart(
formula = Y ~ .,
data    = data,
method  = "anova",
control = list(minsplit = 14, maxdepth = 100, cp = 0.01)
)
pred <- predict(optimal_tree, newdata = data)
RMSE(pred = pred, obs = test$Y)
optimal_tree <- rpart(
formula = Y ~ .,
data    = data,
method  = "anova",
control = list(minsplit = 14, maxdepth = 100, cp = 0.01)
)
pred <- predict(optimal_tree, newdata = data)
RMSE(pred = pred, obs = data$Y)
optimal_tree <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = 14, maxdepth = 100, cp = 0.01)
)
pred <- predict(optimal_tree, newdata = test)
RMSE(pred = pred, obs = test$Y)
hyper_grid <- expand.grid(
minsplit = seq(5, 20, 1),
maxdepth = seq(8, 40, 1)
)
models <- list()
for (i in 1:nrow(hyper_grid)) {
# get minsplit, maxdepth values at row i
minsplit <- hyper_grid$minsplit[i]
maxdepth <- hyper_grid$maxdepth[i]
# train a model and store in the list
models[[i]] <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = minsplit, maxdepth = maxdepth)
)
}
hyper_grid %>%
mutate(
cp    = purrr::map_dbl(models, get_cp),
error = purrr::map_dbl(models, get_min_error)
) %>%
arrange(error) %>%
top_n(-5, wt = error)
get_cp <- function(x) {
min <- which.min(x$cptable[, "xerror"])
cp <- x$cptable[min, "CP"]
}
get_min_error <- function(x) {
min    <- which.min(x$cptable[, "xerror"])
xerror <- x$cptable[min, "xerror"]
}
hyper_grid %>%
mutate(
cp    = purrr::map_dbl(models, get_cp),
error = purrr::map_dbl(models, get_min_error)
) %>%
arrange(error) %>%
top_n(-5, wt = error)
optimal_tree <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = 5, maxdepth = 36, cp = 0.01)
)
pred <- predict(optimal_tree, newdata = test)
RMSE(pred = pred, obs = test$Y)
optimal_tree <- rpart(
formula = Y ~ .,
data    = train,
method  = "anova",
control = list(minsplit = 14, maxdepth = 22, cp = 0.01)
)
pred <- predict(optimal_tree, newdata = test)
RMSE(pred = pred, obs = test$Y)
